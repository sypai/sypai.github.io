<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>syPai</title>
    <description>Disciplined &amp; Relentless</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 31 Aug 2019 19:20:00 +0530</pubDate>
    <lastBuildDate>Sat, 31 Aug 2019 19:20:00 +0530</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Subtitle Resync: Using Audio fingerprinting to synchronize subtitles</title>
        <description>&lt;p&gt;Closed Captions (CC) and subtitles enhance an audiovisual content by providing speech information and description of representative events in a textual format.
Captioning is especially used as an aid for people with hearing loss or deafness, but it’s use
is definitely not limited to that domain. For instance, it is frequently the case that captions are necessary
to watch a TV show in a noisy surrounding or when one is not familiar with the language or accent
available in the audio streams. &lt;br /&gt;&lt;br /&gt;
For an ideal subtitle file, the subtitles are perfectly aligned with the base audiovisual content.
For subtitles to align their timing must align with the corresponding sequence in the audiovisual content.&lt;/p&gt;

&lt;p&gt;There is a very common case in which we have an audiovisual content with some parts of the previous
episode, a lot of commercials throughout and some seconds of the next episode. More commonly a &lt;code class=&quot;highlighter-rouge&quot;&gt;.ts&lt;/code&gt;, a transport
stream file involves such use case. The subtitles extracted from it will contain the subtitle for all the parts including the commercials.
Now we don’t ever use such videos instead we prefer clean recordings devoid of any commercials or breaks in it.
Now using the original subtitles will result in a horrible experience causing a sustained exasperation
to the user.&lt;/p&gt;

&lt;p&gt;The tool &lt;code class=&quot;highlighter-rouge&quot;&gt;Subtitle Resync&lt;/code&gt; takes in the two versions of the same audiovisual
content, the subtitle file for the original video and generates a subtitle file
which is perfectly in-sync with the clean recording. The tool takes about 25 seconds
to create a synced subtitle file so that the viewer does not have any burden before the &lt;em&gt;fun&lt;/em&gt;
starts.&lt;/p&gt;

&lt;blockquote class=&quot;primary link1&quot;&gt;
&lt;em&gt;&lt;a href=&quot;https://github.com/sypai/co-oCCur&quot; target=&quot;_blank&quot;&gt;
         https://github.com/sypai/co-oCCur
       &lt;/a&gt;&lt;/em&gt;
&lt;br /&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
             &lt;h2&gt;&lt;span&gt;Why audio fingerprinting? &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;p&gt;There is lot already written and discussed over the internet on &lt;em&gt;what&lt;/em&gt; is audio fingerprinting. To start
with,
&lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_fingerprint&quot; target=&quot;_blank&quot;&gt;
here&lt;/a&gt; is what wikipedia has to say. An acoustic fingerprint is a condensed digital summary, a fingerprint,
 deterministically generated from an audio signal, that can be used to identify an audio sample.
 &lt;br /&gt;&lt;br /&gt;
 Practical uses of acoustic fingerprinting include identifying songs, melodies, tunes etc. YES! this is
 what the magical
  &lt;a href=&quot;https://www.shazam.com/gb&quot; target=&quot;_blank&quot;&gt;
 Shazam&lt;/a&gt;
 uses to recognize a song from the cosmic corpus of audio.
 &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Our project uses
 &lt;a href=&quot;https://acoustid.org/chromaprint&quot; target=&quot;_blank&quot;&gt;
  chromaprint&lt;/a&gt;, an open source audio fingerprinting library. It works with &lt;em&gt;spectrograms&lt;/em&gt;,
  which are visual representations of the spectrum of frequencies in a sound as these vary with time.
  Chromaprint converts the input audio to the sampling rate of 11025 Hertz and using a FFT(Fast Fourier Transform)
  window size of 4096 with a 2/3 overlap. The algorithm behind chromaprint is complex. Nevertheless,
  the audio fingerprint are array of 32-bit integers. These are hashes of some arbitrary audio features
  over a short period of time. A new hash is generated every 0.1238 seconds and each hash covers
  2.6 seconds of audio.&lt;/p&gt;

&lt;p&gt;The audio fingerprints look like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  array([3208039586, 3207744690, 3207744642, 3177323714, 3187031106,
                2650221586, 2650025010, 2683053106, 2682795042, 2594778150,
                2594909479, 2594782997, 2594709188, 2326298244])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The further processing of these audio fingerprints rarely deal with them in this form,
   instead it can be thought of as a 2D black-and-white image. Black is a 0 bit and white is a 1 bit.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/fp_2d.png&quot; alt=&quot;fp-2D&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
               &lt;h2&gt;&lt;span&gt;How has this anything to do with subtitles? &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
             &lt;h2&gt;&lt;span&gt;Testing the Tool &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;p&gt;For testing the tool, I have used a server which was provided to me by my organization
which has a ample amount of samples. Quantitatively speaking, to test the tool’s speed
and accuracy I have used 16 different valid sample files. Out of the files used
I have got accurate results for 14 samples, 1 sample gave a partial result (perfect results upto 75% of the video) and a sample on which
the tool fails to work. Moreover, it takes an average 26.5 seconds to produce the results
The reason behind the 2 failing results is the section which decides on the basis
of comparison results of fingerprints whether it’s a match, a local match or a no match.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
               &lt;h2&gt;&lt;span&gt;What can be improved? &lt;/span&gt;&lt;/h2&gt;
  &lt;/div&gt;
&lt;p&gt;The project over the course of time will keep improving. The improvements
  that I would like to make in the near future are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Currently for the computation of FFT, I have used kissFFT, a lightweight easy to use open source library.
   I would like to to replace it with FFmpeg’s FFT functions which would make the fingerprint generation process faster.&lt;/li&gt;
  &lt;li&gt;The final steps of deciding the match results definitely are to be improved. As mentioned in the &lt;em&gt;Testing&lt;/em&gt;
   section there are a few cases which are not producing results due to the wrong deduction.
 &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, contribution of any kind is more than welcome. If you have any idea or an issue feel free to open a PR or an issue
in the project repository.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
               &lt;h2&gt;&lt;span&gt;Conclusion &lt;/span&gt;&lt;/h2&gt;
  &lt;/div&gt;
&lt;p&gt;I have developed this project during Google Summer of Code, 2019 which I was fortunate to participate in
  under CCExtractor Development. Obviously a lot more can be done on this but I am glad that finally I
  have an open source project. I am grateful to my mentor &lt;a href=&quot;https://github.com/cfsmp3&quot; target=&quot;_blank&quot;&gt;
  Carlos Fernandez Sanz &lt;/a&gt; who gave me this opportunity and helped me throughout my journey.
  &lt;br /&gt;
  I hope that anyone reading this will check out the &lt;strong&gt;co-oCCur&lt;/strong&gt; project and drop a few starts on me, or better yet, fork it!&lt;/p&gt;
&lt;blockquote class=&quot;primary link1&quot;&gt;
&lt;em&gt;&lt;a href=&quot;https://github.com/sypai/co-oCCur&quot; target=&quot;_blank&quot;&gt;
         https://github.com/sypai/co-oCCur
       &lt;/a&gt;&lt;/em&gt;
&lt;br /&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;
  So long!&lt;/p&gt;

</description>
        <pubDate>Wed, 28 Aug 2019 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/Subtitle-Resync-Using-audio-fingerprinting-to-synchronize-subtitles/</link>
        <guid isPermaLink="true">http://localhost:4000/Subtitle-Resync-Using-audio-fingerprinting-to-synchronize-subtitles/</guid>
        
        
        <category>GSoC</category>
        
      </item>
    
      <item>
        <title>GSoC : Final Work Submission! </title>
        <description>&lt;p&gt;Surreal! My first Google Summer of Code has now come to an end. After almost 5 months
(including the research pre-GSoC) it’s time to pack up. This has been a journey full of learning. This post is to serve the purpose of the final report for the work I have done during the summer.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;Important links &amp;#9939;&lt;/span&gt;&lt;/h2&gt;
 &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Project repository&lt;/strong&gt;  &lt;br /&gt;
 &lt;a href=&quot;https://github.com/sypai/co-oCCur&quot; target=&quot;_blank&quot;&gt;
  co-oCCur @ Github
&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Milestone &amp;amp; deliverable Board&lt;/strong&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trello.com/b/JQGOK4Yo/co-occursubtitlesynchronization&quot; target=&quot;_blank&quot;&gt;
  co-oCCur @ Trello
  &lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Project proposal&lt;/strong&gt;&lt;br /&gt;
  &lt;a href=&quot;https://github.com/sypai/gsoc_19_ccx_proposal/&quot; target=&quot;_blank&quot;&gt;
    co_oCCur/gsoc_19_ccx_proposal
&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mentor&lt;/strong&gt;&lt;br /&gt;
 &lt;a href=&quot;https://github.com/cfsmp3&quot; target=&quot;_blank&quot;&gt;
Carlos Fernandez Sanz @ Github 
&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Organization&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;https://ccextractor.org/&quot; target=&quot;_blank&quot;&gt;
CCExtractor Development
&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;co-oCCur: High-speed subtitle synchronization &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;p&gt;For an ideal subtitle file, the subtitles are perfectly aligned with
 the base audiovisual content. In other words, the audio and the
 corresponding subtitles co-oCCur. 
The misalignment of the subtitle files is the underlying problem
 that this project aims to solve so that the viewer does not have 
any burden before the fun starts (this is what matters).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tool A&lt;/strong&gt; &lt;br /&gt;
Use case: Synchronization of subtitles between two versions
 (with and without commercials) of the same 
 audiovisual content. It takes as input the original audiovisual 
 content, the edited audiovisual content and the subtitles 
 document of the original audiovisual content.&lt;br /&gt;
&lt;strong&gt;Tool B&lt;/strong&gt; &lt;br /&gt;
Use case: Synchronization of subtitles between two versions
of the same audiovisual content in the absence of the original
content. It takes as input the modified audiovisual content
and the subtitle document for the original audiovisual content.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;What has been done? &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;p&gt;The whole project had been divided into modules and sub-modules.
Can be seen in the
&lt;a href=&quot;https://trello.com/b/JQGOK4Yo/co-occursubtitlesynchronization&quot; target=&quot;_blank&quot;&gt;
      Trello
  &lt;/a&gt;
board, the function of each of which I have described in my 
  &lt;a href=&quot;https://github.com/sypai/gsoc_19_ccx_proposal/&quot; target=&quot;_blank&quot;&gt;
     proposal
    &lt;/a&gt;
in great detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SiftSRT&lt;/strong&gt; ✔️&lt;br /&gt;
&lt;a href=&quot;https://github.com/sypai/SiftSRT&quot; target=&quot;_blank&quot;&gt;
SiftSRT: A complete subtitle parser and editor
&lt;/a&gt; (Check it out!).
Whatever our Tool A and Tool B require, related to subtitle files,
SiftSRT has got it covered!. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dactylogram&lt;/strong&gt; ✔️&lt;br /&gt;
The complete 
 &lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_fingerprint&quot; target=&quot;_blank&quot;&gt;
  audio-fingerprinting 
&lt;/a&gt;
 solution for &lt;em&gt;Tool A&lt;/em&gt;, using
&lt;a href=&quot;https://github.com/acoustid/chromaprint&quot; target=&quot;_blank&quot;&gt;
chromaprint
 &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Audio Segmentation&lt;/strong&gt; ✔️&lt;br /&gt;
 Determination of presence of speech in an audio using
 &lt;a href=&quot;https://webrtc.org/&quot; target=&quot;_blank&quot;&gt;
 webrtc
 &lt;/a&gt; for &lt;em&gt;Tool B&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alignment Algorithms&lt;/strong&gt; ️&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tool A&lt;/strong&gt;   ✔&lt;br /&gt;
  Internally, the audio fingerprints that dactylogram generates are
  array of 32-bit integer. We don’t really work with them in this 
  form, but the hashes look something like this:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3594440902&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3594416326&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3598545270&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3598528806&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3615303462&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;3615435574&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3589233430&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3589233495&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3622787783&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3622794949&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;3623867085&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3623862925&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3623867357&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3590189549&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3573346669&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;2499605615&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2498559022&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2490230894&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2490230910&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2490362974&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;These fingerprints are treated usually on bit level. So it’s more useful to think of the 
  fingerprint as a 2D black-and-white image where each white 
  represents a 1 bit and each black pixel is a 0 bit:&lt;/p&gt;

    &lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/fp.png&quot; alt=&quot;fp-2D&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;We use a small subset of bits from the hashes and
  cross-match the two fingerprints. For each hash that appears
  in both fingerprints, we will calculate the difference of offsets 
  at which they appear in the fingerprints. We will build a histogram 
  of these offset differences, do some filtering 
  (effectively estimating a density function using gaussian kernels) and find peaks in it.
  The peaks will tell us how do we need to align the two fingerprints 
  to find matching segments in them.&lt;/p&gt;

    &lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/peaks.png&quot; alt=&quot;peaks&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Using the alignment algorithm our tool detects the multiple segments
  in the original file, separating the actual audiovisual content and the commercials.
Once the segments are identified we use SiftSRT to edit the subtitle document and
  create a perfectly aligned subtitle file.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tool B  &lt;br /&gt;
 For this tool we have two strings containing speech information. 1 represents 
 the presence of speech in th 10ms window while the absence of speech is represented by
 0;
 The proposed algorithm for this tool, in  n simple terms, for each offset, 
 will take a dot product of one string with the offset version of the other.
 Computing this naively would result in an O
 (n^2)​ solution. And “High speed is
 really a priority” is clearly mentioned in the project’s idea page.​ ​ So, we will use
 the Fast Fourier Transform (FFT), bringing the complexity down to​ ​ O(n log n)​ .
 We will score the alignment based on the number of matching 1’s i.e. the
 summation at all values of time of the product of both the strings,
  This is ​ Convolution​. So we can rephrase this problem as, 
  find the index τ which maximizes the value of the convolution of the sequences
  This has been successfully developed but it has not produced results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;Where do we stand? &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;p&gt;At this point, we have a perfectly working Tool A, capable of doing 
exactly what it is supposed to do. &lt;br /&gt; 
&lt;strong&gt;Installation&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Clone the repository from 
&lt;a href=&quot;https://github.com/sypai/co-oCCur&quot; target=&quot;_blank&quot;&gt;
   co-oCCur @ Github
 &lt;/a&gt;
&lt;br /&gt;
    &lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;nt&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;clone&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;://&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;github&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;sypai&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;co-oCCur&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Navigate to &lt;code class=&quot;highlighter-rouge&quot;&gt;install &lt;/code&gt;directory and run &lt;code class=&quot;highlighter-rouge&quot;&gt;build.sh&lt;/code&gt;:&lt;br /&gt;
    &lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;nt&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;install&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.sh&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;The tool is now ready to use!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;br /&gt;
 For a complete list of options and parameters, please
go through the project’s 
&lt;a href=&quot;https://github.com/sypai/co-oCCur/blob/master/README.md&quot; target=&quot;_blank&quot;&gt;
     README
   &lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sync!&lt;br /&gt;
    &lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;   &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;co_oCCur&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-tool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;tool&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;tool&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;specific&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;arguments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/cooccur_wo_args.png&quot; alt=&quot;w/0-args&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tool A&lt;/strong&gt;&lt;br /&gt;
    &lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;co_oCCur&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.wav&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.wav&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.srt&lt;/span&gt; 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;What will this trigger?&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Set Tool A to be used for synchronization.&lt;/li&gt;
      &lt;li&gt;Read the two audio files and generate audio fingerprints for both of them.&lt;/li&gt;
      &lt;li&gt;Using the alignment algorithm for Tool A, detect the different segments.&lt;/li&gt;
      &lt;li&gt;Use the information on segments edit the original subtitle file.&lt;/li&gt;
      &lt;li&gt;Produce a synchronized subtitle file, &lt;em&gt;co_oCCur-original.srt&lt;/em&gt;.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;See it in action:&lt;/p&gt;
    &lt;iframe width=&quot;700&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/i_HQIUoM6E&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tool B&lt;/strong&gt;&lt;br /&gt;
    &lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;   &lt;span class=&quot;o&quot;&gt;./&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;co_oCCur&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.wav&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;subtitle&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.srt&lt;/span&gt; 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;What happens next?&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Set B as the tool to be used for synchronization.&lt;/li&gt;
      &lt;li&gt;Read the audio file and create the audio string.&lt;/li&gt;
      &lt;li&gt;Parse the subtitle file and create the subtitle string.&lt;/li&gt;
      &lt;li&gt;Using the alignment algorithm for Tool B, detect the different blocks.&lt;/li&gt;
      &lt;li&gt;Use the information on segments edit the original subtitle file.&lt;/li&gt;
      &lt;li&gt;Produce a synchronized subtitle file, &lt;em&gt;co_oCCur-original.srt&lt;/em&gt;.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;Check out what really happens!&lt;/p&gt;
    &lt;iframe width=&quot;700&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/i_HQIUoM6E&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;What else can been done? &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;p&gt;The project is in it’s early stage and will keep evolving. The available functions, usage instructions et cetera are 
expected to refactor over time. There are several ideas and features that can be added to this project.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Currently only SRT is the subtitle format that this project supports. Support for other formats can be added.&lt;/li&gt;
  &lt;li&gt;The misalignment of subtitles because of different encoding settings in videos causes a varying temporal offset. This is
also a great feature to add in to our project. &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, contribution of any kind is more than welcome. Feel free to raise an issue tracker here: 
&lt;a href=&quot;https://github.com/sypai/co-oCCur/issues&quot; target=&quot;_blank&quot;&gt;
   here
   &lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;GSoC: Endgame &lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;p&gt;Before the final goodbyes, I would like to thank my mentor, 
&lt;a href=&quot;https://github.com/cfsmp3&quot; target=&quot;_blank&quot;&gt;
Carlos Fernandez Sanz 
&lt;/a&gt; 
for accepting my proposal, 
providing guidance whenever I asked, and believing in me when things didn’t go as intended. I am grateful. 
&lt;br /&gt;Shout out to the CCExtractor community for being so welcoming and supportive. 
I am fortunate to be a part of the 
&lt;a href=&quot;https://ccextractor.org&quot; target=&quot;_blank&quot;&gt;
CCExtractor Development 
&lt;/a&gt;
&lt;br /&gt;
End? No, the journey doesn’t end here, it begins.. I’m sure I will stick around as a regular contributor.&lt;/p&gt;

&lt;blockquote class=&quot;primary right&quot;&gt;
&lt;em&gt;&quot;I pledge my life and honor to the Night's Watch, for this night and all the nights to come.&quot;&lt;/em&gt;
&lt;br /&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Sun, 25 Aug 2019 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/GSoC-Final-work-submissions/</link>
        <guid isPermaLink="true">http://localhost:4000/GSoC-Final-work-submissions/</guid>
        
        
        <category>GSoC</category>
        
      </item>
    
      <item>
        <title>GSoC [02]: Basic needs fulfilled! </title>
        <description>&lt;p&gt;The first of the three phases of coding is about to end. And
 the evaluations for this phase start from 24th June.
This post contains the synopsis of my work during the
 first four weeks.&lt;/p&gt;

&lt;!-- Where ?--&gt;
&lt;div class=&quot;section-title margtop3rem&quot;&gt;
             &lt;h2&gt;&lt;span&gt;Where do we want to be? &amp;#127768;&lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;

&lt;p&gt;A question we need an answer to, before starting anything. 
 Let’s start with that. It won’t be fair if I talk a blue streak 
 about &lt;em&gt;what I have&lt;/em&gt; done without discussing &lt;em&gt;what I want to do&lt;/em&gt;.
 &lt;br /&gt;
 The project co-oCCur aims to address the 
 issue: “misalignment of subtitles with the base audiovisual
  content”. There are two use cases that the project currently
   aims to tackle leveraging two different approaches using
    two tools, namely &lt;strong&gt;Tool A&lt;/strong&gt; and &lt;strong&gt;Tool B&lt;/strong&gt;. 
    My
 &lt;a href=&quot;https://docs.google.com/document/d/1QEB3jDNvFVPP4OpvU1m_A3kME288LZKJVIs1g1AXin4/edit?usp=sharing&quot;&gt;
  proposal
 &lt;/a&gt;
 elucidates the approach of tackling both the cases and the 
 architecture of both the tools in depth. In a nutshell, 
 Tool A will use 
 &lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_fingerprint&quot;&gt;
    audio-fingerprint 
  &lt;/a&gt;
 annotations while Tool B will create two strings
  (an audio string and a subtitle string), to detect a 
  &lt;strong&gt;&lt;em&gt;constant temporal offset&lt;/em&gt;&lt;/strong&gt; responsible for the misalignment. 
 Adjustment of the subtitles will be achieved by propagating 
 a time delay in each of the caption entries in the subtitle 
 document. The resulting subtitle document will have a 
 different presentation time for each caption entry so that the
 audio and the subtitles &lt;strong&gt;co-oCCur&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tool A&lt;/strong&gt; &lt;br /&gt;
 Use case: Synchronization of subtitles between two versions 
 (for example, with and without commercials) 
 of the same audiovisual content. It will take as input the 
 original audiovisual content, the edited audiovisual content 
 and the subtitles document of the original audiovisual content.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tool B&lt;/strong&gt; &lt;br /&gt;
 Use case: Synchronization of subtitles between two versions
 of the same audiovisual content in the absence of the 
 original content. It will take as input the modified audiovisual 
 content and the subtitle document for the original 
 audiovisual content.&lt;/p&gt;

&lt;p&gt;The whole project has been divided into modules and
  sub-modules. The function of each of which I have described
   in my proposal in great detail. During the first coding 
   phase, I had decided to work on all the basic utility modules.&lt;/p&gt;

&lt;p&gt;&lt;!-- How along ?--&gt;&lt;/p&gt;
&lt;div class=&quot;section-title margtop3rem&quot;&gt;
              &lt;h2&gt;&lt;span&gt;How's the Subtitle Synchronization coming along? ️️&amp;#9203;&lt;/span&gt;&lt;/h2&gt;
 &lt;/div&gt;

&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/phase1_tasks.png&quot; alt=&quot;Trello Board&quot; /&gt;
These were the proposed deliverables for the first evaluations. 
 I have also created a &lt;em&gt;Trello&lt;/em&gt; board  to make it easier for my mentor and myself, to keep a track of my progress.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Project link&lt;/em&gt;&lt;/strong&gt;:  &lt;a href=&quot;https://github.com/sypai/co-oCCur&quot; target=&quot;_blank&quot;&gt;
   github.com/sypai/co-oCCur 
   &lt;/a&gt;&lt;br /&gt;
   &lt;strong&gt;&lt;em&gt;Trello&lt;/em&gt;&lt;/strong&gt;: &lt;a href=&quot;https://trello.com/b/JQGOK4Yo/co-occursubtitlesynchronization&quot; target=&quot;_blank&quot;&gt;
   trello.com/co-occursubtitlesynchronization
  &lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As I had mentioned in the 
  &lt;a href=&quot;GSoC-00-Bonding-period-ends-&amp;amp;-Coding-begins-!/&quot; target=&quot;_blank&quot;&gt;
  last post&lt;/a&gt;, my final examinations 
  started from the very same day the coding period began, which made 
  everything a bit cluttery in the first two weeks. I could not 
  devote the time I planned and needed to meet the deadlines.
 As a result, tasks were getting postponed. But somehow after
a few sleepless nights, lots of coffee and an endless loop of
good music later, things have got on track, at least I feel so.&lt;/p&gt;

&lt;p&gt;Here are the tasks I checked off during the &lt;em&gt;first coding phase&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sample Repository&lt;/strong&gt; ✔️ &lt;br /&gt;
The first task in hand was to collect samples, a lot of samples. 
I had mentioned this as a requirement in my proposal. 
I informed my mentor @&lt;a href=&quot;https://github.com/cfsmp3&quot;&gt;cfsmp3
&lt;/a&gt;
 and he gave me access to 
a high-speed development server (gsocdev3) which has a 
huge sample collection. &lt;em&gt;FFmpeg&lt;/em&gt; and &lt;strong&gt;&lt;em&gt;CCExtractor&lt;/em&gt;&lt;/strong&gt; are the 
tools I used to extract audio and subtitles &amp;amp; collected them 
in a &lt;code class=&quot;highlighter-rouge&quot;&gt;SampleRepository&lt;/code&gt;. For our project, the audio needs to 
have certain specifications. It needs to be uncompressed, 
raw PCM (16 bit-signed int), mono sampled at 16000 Hertz.
&lt;br /&gt;
&lt;br /&gt;
&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/samplerepo.png&quot; alt=&quot;Sample Repo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Basic Architecture&lt;/strong&gt; ✔️&lt;br /&gt;
This is how the &lt;a href=&quot;https://github.com/sypai/co-oCCur&quot;&gt;co-oCCur&lt;/a&gt; repo currently look like:
&lt;br /&gt;&lt;br /&gt;
&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/proj_structure.png&quot; alt=&quot;Project tree&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base64 &lt;code class=&quot;highlighter-rouge&quot;&gt;encrypt&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;decrypt&lt;/code&gt;&lt;/strong&gt; ✔️&lt;br /&gt;
[&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool A&lt;/code&gt;] The fingerprints extracted from the audio will be 32-bit integer
 vectors and this is the form we will use while comparing them later.
But in the subtitle document they will be stored as base64 strings.
Task of converting these integer vectors to base64 strings and vice versa
has been completed.
&lt;br /&gt;
&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/base64.png&quot; alt=&quot;base64&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SiftSRT&lt;/strong&gt; ✔️&lt;br /&gt;
&lt;a href=&quot;https://github.com/sypai/SiftSRT&quot; target=&quot;_blank&quot;&gt;
SiftSRT: A complete subtitle parser and editor
&lt;/a&gt; (Check it out!).
Whatever our Tool A and Tool B require, related to subtitle files,
 SiftSRT has got it covered!. &lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    &lt;span class=&quot;n&quot;&gt;SubtitleParserFactory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;spf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SubtitleParserFactory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inputSubtitle.srt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;SubtitleParser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SubtitleItem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getSubtitles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;co_oCCurEditor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;edit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;co_oCCurEditor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  
    &lt;span class=&quot;n&quot;&gt;co_oCCurParser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;co_oCCurParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AFInserter&lt;/strong&gt; &lt;br /&gt; 
 [&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool A&lt;/code&gt;] Our audio fingerprinting library &lt;strong&gt;Dactylogram&lt;/strong&gt; 
 (Phase 2) will extract audio fingerprints annotations from the original audio
 and then they will be inserted into the original subtitle 
 document as fingerprint anchors and 
 create an enriched subtitle document.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;n&quot;&gt;edit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EnrichSRT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;temp.srt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fingerprints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AFSeeker&lt;/strong&gt; &lt;br /&gt;
[&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool A&lt;/code&gt;] For comparing our fingerprints extracted from modified audio
with the fingerprint anchors we will need the anchors. Our Subtitle parser
will give that.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FPs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getFingerprints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FPTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getFPTimestamps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Subtitle Segmentation&lt;/strong&gt; &lt;br /&gt;
[&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool B&lt;/code&gt;] As mentioned we need a subtitleString containing information about subtitles
&lt;br /&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;1 = Subtitle present in the window&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;0 = No subtitle in the window&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;substring&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SpeechActivityDetection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Subtitle Adjustment&lt;/strong&gt; &lt;br /&gt;
 [&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool A&lt;/code&gt;][&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool B&lt;/code&gt;] The last step in both tools is adjusting the subtitle file 
 using the constant temporal offset detected by respective algorithm.
 As we get the &lt;code class=&quot;highlighter-rouge&quot;&gt;delay&lt;/code&gt; we propagate it to each caption entry.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;edit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdjustSRT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;example.srt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;VAD Implementation&lt;/strong&gt; ✔️&lt;br /&gt;
[&lt;code class=&quot;highlighter-rouge&quot;&gt;Tool B&lt;/code&gt;] Out of the required two strings for alignment &lt;code class=&quot;highlighter-rouge&quot;&gt;audioString&lt;/code&gt; contains information
of voice activity in a time window for the modified audio. 
This requires  &lt;strong&gt;&lt;em&gt;Voice Activity Detection&lt;/em&gt;&lt;/strong&gt;. Google’s open-source 
&lt;a href=&quot;https://webrtc.org/&quot; target=&quot;_blank&quot;&gt;WebRTC&lt;/a&gt; has a VAD
module which uses Gaussian Mixture Model (GMM). I have used that for creating our
&lt;code class=&quot;highlighter-rouge&quot;&gt;audioString&lt;/code&gt;.
&lt;br /&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;1 = Subtitle present in the window&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;0 = No subtitle in the window&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int16_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getSamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;implementVAD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!-- Fall into place --&gt;
&lt;div class=&quot;section-title margtop3rem&quot;&gt;
              &lt;h2&gt;&lt;span&gt;How does everything fall into place? 🧩&lt;/span&gt;&lt;/h2&gt;
 &lt;/div&gt;
&lt;p&gt;All the &lt;strong&gt;&lt;em&gt;basic needs&lt;/em&gt;&lt;/strong&gt; of the project have been fulfilled.&lt;br /&gt;
 The figures below describe the workflow of the Tools. 
 ✔️ represents which parts of the workflow have been completed.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
                &lt;h3&gt;&lt;span&gt; Tool A &lt;/span&gt;&lt;/h3&gt;
   &lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/Tool A (2).png&quot; alt=&quot;Project tree&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
                 &lt;h3&gt;&lt;span&gt; Tool B &lt;/span&gt;&lt;/h3&gt;
    &lt;/div&gt;
&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/Tool B (2).png&quot; alt=&quot;Project tree&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
 &lt;h2&gt;&lt;span&gt;Road ahead &amp;#128739;&lt;/span&gt;&lt;/h2&gt;
 &lt;/div&gt;
&lt;p&gt;Now I will begin working on &lt;strong&gt;&lt;em&gt;Dactylogram&lt;/em&gt;&lt;/strong&gt;, the most interesting, challenging and
presumably time-taking part of the whole project. The goal is to develop
an audio fingerprinting library required for &lt;strong&gt;Tool A&lt;/strong&gt;. In the workflow above, the
blocks named &lt;strong&gt;&lt;em&gt;AFExtractor&lt;/em&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;em&gt;Seeking&lt;/em&gt;&lt;/strong&gt; would be the &lt;code class=&quot;highlighter-rouge&quot;&gt;TO-DO&lt;/code&gt; for the 
second phase.
&lt;br /&gt;&lt;br /&gt;
All the best to all the participants for the first evaluations. 
I hope all of us pass with flying colors. 🤞 👍
&lt;br /&gt; &lt;br /&gt;
See you on the other side!&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Jun 2019 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/GSoC-02-Basic-needs-fulfilled!/</link>
        <guid isPermaLink="true">http://localhost:4000/GSoC-02-Basic-needs-fulfilled!/</guid>
        
        
        <category>GSoC</category>
        
      </item>
    
      <item>
        <title>GSoC [01]: Bonding period ends &amp; Coding begins! </title>
        <description>&lt;p&gt;A month-long community bonding period ended on 27 May,
commencing the official coding period. In this post, I would
like to share my GSoC 2019 community bonding experience
with CCExtractor Development, work-progress and the road
ahead.&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;Community Bonding Period&lt;/span&gt;&lt;/h2&gt;
 &lt;/div&gt;

&lt;p&gt;The official list of selected proposals was released by Google
 on the 6th of May and with it started the community bonding
 period. It began with me &lt;em&gt;jumping for joy&lt;/em&gt; after getting the
 news of my proposal getting selected.&lt;/p&gt;

&lt;p&gt;It was basically a month full of learning and planning.
 Two social groups have been created, one on Facebook
 and the other on Telegram, which includes over 600 selected
 students from around the globe. It is a great platform for
 sharing your progress and getting help when you are stuck
  somewhere.&lt;/p&gt;

&lt;p&gt;Here are the tasks I checked off during the community period!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Deciding the structure of my project&lt;/strong&gt;&lt;br /&gt;
This is the first time I am going to be working on a project
 which involves so much! Deciding the structure of the project
early was a crucial task so that the development stays
smooth ahead. I was sure to use C++ as the language
and thus I started off with strengthening the concepts
of the language. After checking out some popular C++
projects and reading some blogs on the same I have
decided to use something like this:
&lt;br /&gt;&lt;br /&gt;
 &lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/project_structure.png&quot; alt=&quot;Structure Project&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Creating a Trello board&lt;/strong&gt;&lt;br /&gt;
As I had mentioned in my proposal I wanted to have a
detailed checklist of the tasks to be done with their deadlines.
This would help me and my mentor to keep track of my work.
 I am maintaining the milestones and deliverables in a
 Trello board, which can be found here : &lt;br /&gt;
 &lt;a href=&quot;https://trello.com/b/JQGOK4Yo/co-occursubtitlesynchronization&quot; target=&quot;_blank&quot;&gt;
 Milestones and checklist for co-oCCur: High-speed subtitle synchronization tool
 &lt;/a&gt;&lt;br /&gt;&lt;br /&gt;
 &lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/trello_cooCCur.png&quot; alt=&quot;Trello Board&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Setting-up my Blog&lt;/strong&gt;&lt;br /&gt;
For everyone to follow my progress on the project I have set
 up this blog. This is the best way for the whole community
to know what’s cooking.
It’s a simple and 
&lt;a href=&quot;https://www.wowthemes.net/mediumish-free-jekyll-template/&quot; target=&quot;_blank&quot;&gt;
 mediumish
 &lt;/a&gt;
themed Jekyll blog hosted on
Github pages. &lt;br /&gt; &lt;br /&gt;
Here it resides: &lt;a href=&quot;https://sypai.github.io&quot; target=&quot;_blank&quot;&gt;https://sypai.github.io &lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Setting-up Payoneer account&lt;/strong&gt;&lt;br /&gt;
There have been more discussions on this in the Telegram
 group than anything else. Fortunately, it took me around ten
 minutes to set it up and receive the confirmation email from
Payoneer.
 &lt;br /&gt;&lt;br /&gt;
 &lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/pay.png&quot; alt=&quot;Payoneer&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;h2&gt;Surprise &amp;#127873;  &amp;#128217; &lt;/h2&gt;&lt;/p&gt;
&lt;p&gt;As mentioned in the CCExtractor’s website,&lt;/p&gt;

&lt;blockquote class=&quot;primary left&quot;&gt;
&lt;em&gt;This year we're going to try something new. All
 accepted students will get a programming book
  immediately after being accepted, with the hope that they
  read them before the coding starts. We want to see
  if this increases the quality of the work.&lt;/em&gt;
&lt;br /&gt;
&lt;/blockquote&gt;
&lt;p&gt;Each of the accepted students was to choose a technical
 book of their choice. I chose &lt;strong&gt;&lt;em&gt;Elements of Programming
 Interviews in Python&lt;/em&gt;&lt;/strong&gt; by &lt;em&gt;Adnan Aziz&lt;/em&gt;, &lt;em&gt;Tsung-Hsien Lee&lt;/em&gt; and &lt;em&gt;Amit Prakash&lt;/em&gt;. Placement season being next
 in the queue, this book is a very good addition to my
 library.
 &lt;br /&gt; Thank you &lt;a href=&quot;https://ccextractor.org&quot; target=&quot;_blank&quot;&gt;CCExtractor Development!&lt;/a&gt; 😍
&lt;br /&gt;&lt;br /&gt;
   &lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/book_CC.jpg&quot; alt=&quot;Book&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;h2&gt;Bad News &amp;#128531;&lt;/h2&gt;&lt;/p&gt;
&lt;p&gt;My end semester exams have been postponed, this was
 known, I had mentioned this in my proposal. But the final 
 schedule wasn’t announced until 20th May. Guess what! 
 Exams started from the very same day the coding period 
 officially began. 
 &lt;br /&gt;
I had to re-schedule the weekly milestones and deadlines.
 I have informed my mentor regarding this and he says,&lt;/p&gt;

&lt;blockquote class=&quot;primary left&quot;&gt;
&lt;em&gt;
as long as you are on track by the time of the first evaluation 
you're OK. But it would be a good idea to show me your
 progress often.
&lt;/em&gt;
&lt;br /&gt;
&lt;/blockquote&gt;

&lt;p&gt;It would be a bit of a predicament to manage to work with 
the exams. I hope by the end of first evaluation, I will able
 to complete all the deliverables.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;h2&gt;Road Ahead! &amp;#128739; &lt;/h2&gt;&lt;/p&gt;
&lt;p&gt;At the end of the first four weeks, first evaluation [24 May - 28 May] begins.
 I will try to stick to the deadlines as I have mentioned in the 
 &lt;a href=&quot;https://trello.com/b/JQGOK4Yo/co-occursubtitlesynchronization&quot; target=&quot;_blank&quot;&gt;
 trello &lt;/a&gt; 
 board. I know it is going to be a roller-coaster ride. I am sure I will learn a lot during the next 3 months!&lt;/p&gt;

&lt;p&gt;I wish all the participants a fun and a productive summer. Happy Coding!&lt;/p&gt;

</description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/GSoC-00-Bonding-period-ends-&-Coding-begins-!/</link>
        <guid isPermaLink="true">http://localhost:4000/GSoC-00-Bonding-period-ends-&-Coding-begins-!/</guid>
        
        
        <category>GSoC</category>
        
      </item>
    
      <item>
        <title>GSoC [00]: And now my watch begins!</title>
        <description>&lt;p&gt;My proposal
 &lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#5538361094176768&quot; target=&quot;_blank&quot;&gt;co-oCCur: High-speed subtitle synchronization&lt;/a&gt;
 has been accepted by 
 &lt;a href=&quot;https://ccextractor.org&quot; target=&quot;_blank&quot;&gt;CCExtractor Development&lt;/a&gt;
 for Google Summer of Code (GSoC) 2019!
  &lt;br /&gt;
 I am thrilled to bits to spend my summer working with the “magicians” 
 behind the de-facto subtitle tool - &lt;a href=&quot;https://github.com/CCExtractor/ccextractor&quot; target=&quot;_blank&quot;&gt;CCExtractor&lt;/a&gt;.
 My mentor is &lt;a href=&quot;https://github.com/cfsmp3&quot; target=&quot;_blank&quot;&gt;Carlos Fernandez Sans &lt;/a&gt;(org admin, who originally built CCExtractor). What more could one ask for?&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;About GSoC&lt;/span&gt;&lt;/h2&gt;
        &lt;/div&gt;
&lt;p&gt;
&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/what_is_gsoc.png&quot; alt=&quot;What is GSoC&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Simply put, it is an international annual program by Google aimed at promoting 
&lt;a href=&quot;https://opensource.com/resources/what-open-source&quot; target=&quot;_blank&quot;&gt;
Open Source Software development &lt;/a&gt; 
among college and university students. Students apply for a project/idea of their choice to one of the many 
&lt;a href=&quot;https://summerofcode.withgoogle.com/organizations/?sp-page=2&quot; target=&quot;_blank&quot;&gt;
Open Source Organizations&lt;/a&gt;
selected by Google, by submitting a proposal. Selected students spend their summer building the project by implementing what they proposed.
&lt;br /&gt;
&lt;br /&gt;
In return, the students gain excellent experience of building a real-world project, mentored by seasoned developers. The stipend 💰 and of course the bragging rights! 😎&lt;/p&gt;

&lt;div class=&quot;section-title margtop3rem&quot;&gt;
            &lt;h2&gt;&lt;span&gt;About my project&lt;/span&gt;&lt;/h2&gt;
        &lt;/div&gt;

&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/GSoC_myProject.png&quot; alt=&quot;My Project on GSoC Website&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Out of the many ideas, present in the 
&lt;a href=&quot;https://ccextractor.org/public:gsoc:ideas_page_for_summer_of_code_2019&quot; target=&quot;_blank&quot;&gt;CCExtractor’s GSoC page&lt;/a&gt;, 
I gravitated to
 &lt;a href=&quot;https://ccextractor.org/public:gsoc:highspeedsync&quot; target=&quot;_blank&quot;&gt;Writing high-Speed subtitle synchronization tool&lt;/a&gt;.
After about a month of research on the same, I submitted my proposal on the 9th of April. 
&lt;a href=&quot;https://docs.google.com/document/d/1QEB3jDNvFVPP4OpvU1m_A3kME288LZKJVIs1g1AXin4/edit?usp=sharing&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;
 is my accepted proposal!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Closed_captioning&quot; target=&quot;_blank&quot;&gt;Closed Captions (CC)&lt;/a&gt;
 and subtitles enhance audiovisual content by providing speech information and description of representative events in a textual format. 
 Captioning is especially used as an aid for people with hearing loss or deafness, but its use is definitely not limited to that domain.
 &lt;br /&gt; 
 For instance, it is frequently the case that captions are necessary to watch a TV show in a noisy surrounding or when one is not familiar 
 with the language or accent available in the audio streams.
 &lt;br /&gt;
 &lt;br /&gt;
Downloading a subtitle document and playing it alongside an episode of your favourite TV show, is not rocket science, 
but it sure can feel that way sometimes. 
Getting the subtitle document that gives satisfactory synchronization on the first attempt is like hitting the jackpot.
 I know the annoyance that comes in with misaligned subtitles and is a very general problem. 
 &lt;br /&gt;
 For an ideal subtitle file, the subtitles are perfectly aligned with the base audiovisual content. In other words, the audio and the corresponding subtitles 
 &lt;b&gt;&lt;u&gt;co-oCCur&lt;/u&gt;&lt;/b&gt;.
 &lt;br /&gt;
The misalignment of the subtitle files is the underlying problem that this project aims to solve so that the viewer does not have any burden before the fun starts (this is what matters).&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;featured-image img-fluid&quot; src=&quot;/assets/images/timeline.png&quot; alt=&quot;GSoC Timeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the next 3 months, I will be working on this amazing project. I hope by the end of this summer the tool is ready to be used!&lt;/p&gt;

&lt;blockquote class=&quot;primary right&quot;&gt;

&lt;em&gt;&quot;Every skill you acquire, doubles your odds of success&quot;&lt;/em&gt;
&lt;br /&gt;
&lt;small&gt;
&lt;cite title=&quot;Source Title&quot;&gt;Scott Adams&lt;/cite&gt;
&lt;/small&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sun, 19 May 2019 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/Google-Summer-of-Code-And-now-my-watch-begins/</link>
        <guid isPermaLink="true">http://localhost:4000/Google-Summer-of-Code-And-now-my-watch-begins/</guid>
        
        
        <category>GSoC</category>
        
      </item>
    
  </channel>
</rss>
